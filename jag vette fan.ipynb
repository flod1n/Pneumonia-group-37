{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torch import flatten, optim\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score,f1_score\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is: cuda\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 7\n",
    "SHUFFLE = True\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 1\n",
    "\n",
    "CUDA = True\n",
    "if CUDA and torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else: device = \"cpu\"\n",
    "print(\"device is:\",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f'predicted: {class_names[preds[j]]}')\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Display image for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "def labelcounter(dataloader):\n",
    "    pos,neg = 0,0\n",
    "    for pic, label in dataloader:\n",
    "        label_values = label.tolist()\n",
    "        for label_value in label_values:\n",
    "            if label_value == 0:\n",
    "                neg += 1\n",
    "            else:\n",
    "                pos += 1\n",
    "    return pos,neg\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100 * correct / total\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        conf_matrix = confusion_matrix(all_targets, all_predictions)\n",
    "        # Calculate additional metrics\n",
    "        precision = precision_score(all_targets, all_predictions, average='weighted')\n",
    "        recall = recall_score(all_targets, all_predictions, average='weighted')\n",
    "        f1 = f1_score(all_targets, all_predictions, average='weighted')\n",
    "\n",
    "    return test_loss, accuracy, precision, recall, f1, conf_matrix\n",
    "\n",
    "def make_weights_for_balanced_classes(images, nclasses):\n",
    "    n_images = len(images)\n",
    "    count_per_class = [0] * nclasses\n",
    "    for _, image_class in images:\n",
    "        count_per_class[image_class] += 1\n",
    "    weight_per_class = [0.] * nclasses\n",
    "    for i in range(nclasses):\n",
    "        weight_per_class[i] = float(n_images) / float(count_per_class[i])\n",
    "    weights = [0] * n_images\n",
    "    for idx, (image, image_class) in enumerate(images):\n",
    "        weights[idx] = weight_per_class[image_class]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_DIR = r\"C:\\Users\\ludvi\\Documents\\Code\\tjena\\chest_xray\\train\"\n",
    "TRAIN2_DATA_DIR = r\"C:\\Users\\ludvi\\Documents\\Code\\tjena\\chest_xray\\train2\"\n",
    "VAL_DATA_DIR = r\"C:\\Users\\ludvi\\Documents\\Code\\tjena\\chest_xray\\val\"\n",
    "TEST_DATA_DIR = r\"C:\\Users\\ludvi\\Documents\\Code\\tjena\\chest_xray\\test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize(size=(224,224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "transform2 = transforms.Compose([\n",
    "        transforms.Resize(size=(224,224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "# transform = v2.Compose([\n",
    "#         v2.Resize(size=(224,224)),\n",
    "#         v2.RandomHorizontalFlip(p=0.5),\n",
    "#         v2.ToTensor(),\n",
    "#         v2.ToDtype(torch.float32, scale=True),  # Normalize expects float input\n",
    "#         v2.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                             std=[0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "# Load dataset\n",
    "train_set = datasets.ImageFolder(root=TRAIN_DATA_DIR, transform=None)\n",
    "val_set = datasets.ImageFolder(root=VAL_DATA_DIR,transform=transform)\n",
    "test_set = datasets.ImageFolder(root=TEST_DATA_DIR,transform=transform)\n",
    "train_set_2 = datasets.ImageFolder(root=TRAIN_DATA_DIR, transform=transform2)\n",
    "\n",
    "# Create data loaders for train, validation, and test sets\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "train_loader_2 = DataLoader(train_set_2, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "transToPIL = v2.ToPILImage()\n",
    "img = transToPIL(train_set_2[0][0])\n",
    "img.show()\n",
    "\n",
    "\n",
    "# print(\"train set size:\",len(train_set))\n",
    "# print(\"val set size:\",len(val_set))\n",
    "# print(\"test set size:\",len(test_set))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pos,train_neg = labelcounter(train_loader)\n",
    "# val_pos,val_neg = labelcounter(val_loader)\n",
    "# test_pos,test_neg = labelcounter(test_loader)\n",
    "\n",
    "# species = ('train', 'val', 'test')\n",
    "# balance = {\n",
    "#     'positive': np.array([train_pos, val_pos, test_pos]),\n",
    "#     'negative': np.array([train_neg, val_neg, test_neg]),\n",
    "# }\n",
    "# width = 0.7  # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# bottom = np.zeros(3)\n",
    "\n",
    "# for positive, negative in balance.items():\n",
    "#     p = ax.bar(species, negative, width, label=positive, bottom=bottom)\n",
    "#     bottom += negative\n",
    "\n",
    "#     ax.bar_label(p, label_type='center')\n",
    "\n",
    "# ax.set_title('Dataset balance')\n",
    "# ax.legend()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
    "    best_val_loss = 1000\n",
    "    best_model = model\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            # labels = labels.unsqueeze(-1) # -1 stands for last here equivalent to 1\n",
    "            # labels = labels.unsqueeze(1)\n",
    "            labels = torch.squeeze(labels)\n",
    "            labels = torch.float()\n",
    "            # print(\"labels\",labels)\n",
    "            # print(\"pred\", outputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss.to(device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        writer.add_scalar(\"loss/train\", train_loss, epoch)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = torchvision.models.resnet101(pretrained=True)\n",
    "        modules = list(self.resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.dem_red1 = nn.Conv2d(2048, 1024, kernel_size=1, padding=0)\n",
    "        self.dem_red2 = nn.Conv2d(1024, 512, kernel_size=1, padding=0)\n",
    "        self.fc1 = nn.Linear(512*7*7,1000)\n",
    "        self.act = nn.ReLU()\n",
    "        self.act2 = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(1000,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dem_red1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dem_red2(x)\n",
    "        x = self.act(x)\n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.fc2(x)\n",
    "        x= self.act2(x)\n",
    "        # x = torch.squeeze(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wtf(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.RELU = nn.ReLU()\n",
    "        self.Tan = nn.Tanh()\n",
    "\n",
    "        self.densenet = torchvision.models.densenet.DenseNet()\n",
    "        \n",
    "        # input: 32x32x1\n",
    "        self.c1 = nn.Conv2d(1, 3, kernel_size=3, padding=1) # output: 32x32x3 SAME\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 16x16x3 \n",
    "        self.c2 = nn.Conv2d(3, 6, kernel_size=3, padding=1) # output: 16x16x6 SAME\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 8x8x6 \n",
    "        self.c3 = nn.Conv2d(6, 12, kernel_size=3, padding=1) # output: 8x8x12 SAME\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=8*8*12, out_features=60) \n",
    "\n",
    "        self.logSoftmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #Conv Layer 1\n",
    "        x = self.c1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool1(x)\n",
    "        #Conv Layer 2        \n",
    "        x = self.c2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool2(x)\n",
    "        #Conv Layer 3\n",
    "        x = self.c3(x)\n",
    "        x = self.act(x)\n",
    "        #Flattening\n",
    "        x = torch.flatten(x, 1)\n",
    "        #FC Layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        #FC Layer 2\n",
    "        x = self.fc2(x)    \n",
    "        #Softmax\n",
    "        out = self.logSoftmax(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferLearning(nn.Module):\n",
    "    def __init__(self, my_pretrained_model):\n",
    "        super(TransferLearning, self).__init__()\n",
    "        self.pretrained = my_pretrained_model\n",
    "        self.my_new_layers = nn.Sequential(nn.Linear(1000, 100),\n",
    "                                            nn.Softmax(),\n",
    "                                            nn.Linear(100, 1)) # Binary classification?\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pretrained(x)\n",
    "        x = self.my_new_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ludvi\\anaconda3\\envs\\Jaha\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ludvi\\anaconda3\\envs\\Jaha\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1/1:   0%|          | 0/746 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'torch.dtype' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params\u001b[38;5;241m=\u001b[39mresnet2\u001b[38;5;241m.\u001b[39mparameters(),lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m     11\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCELoss()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 12\u001b[0m trained_model1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m test_loss, accuracy, precision, recall, f1, conf_matrix \u001b[38;5;241m=\u001b[39m test_model(trained_model1, test_loader,criterion)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m,accuracy)\n",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# labels = labels.unsqueeze(-1) # -1 stands for last here equivalent to 1\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# labels = labels.unsqueeze(1)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(labels)\n\u001b[1;32m---> 16\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# print(\"labels\",labels)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# print(\"pred\", outputs)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'torch.dtype' object is not callable"
     ]
    }
   ],
   "source": [
    "# resnet = TransferLearning(torchvision.models.resnet101(pretrained=False)).to(device)\n",
    "# optimizer = torch.optim.Adam(params=resnet.parameters(),lr=0.0001)\n",
    "# criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# densenet = TransferLearning(torchvision.models.densenet.densenet121()).to(device)\n",
    "# optimizer = torch.optim.Adam(params=resnet.parameters(),lr=0.0001)\n",
    "# criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "resnet2 = Encoder().to(device)\n",
    "optimizer = torch.optim.Adam(params=resnet2.parameters(),lr=0.0001)\n",
    "criterion = torch.nn.BCELoss().to(device)\n",
    "trained_model1 = train_model(resnet2,criterion,optimizer,train_loader_2,val_loader,num_epochs=EPOCHS)\n",
    "test_loss, accuracy, precision, recall, f1, conf_matrix = test_model(trained_model1, test_loader,criterion)\n",
    "\n",
    "print(\"Accuracy:\",accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\",f1)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_confusion_matrix(conf_matrix, classes=['Negative', 'Positive'], normalize=False)\n",
    "plt.show()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 1: Resnet med aug\n",
    "\n",
    "![alt text](image.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jaha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
